# Research Findings: How tutor-gpt Formats Honcho Working Representations

## Executive Summary

**Key Discovery**: tutor-gpt does NOT send raw JSON to the LLM. Instead, it:
1. Calls `honcho.apps.users.sessions.chat()` which returns `{ content: string }` - a **plain text string**
2. Wraps this string in **XML tags**: `<context>${honchoContent}</context>`
3. Injects it into the LLM prompt at specific positions

This is fundamentally different from sending raw JSON objects like:
```json
{"representation": {"explicit": [...], "deductive": [...]}}
```

---

## 1. WHERE WORKING REPRESENTATIONS ARE RETRIEVED

### Primary Retrieval Method

**File**: `utils/ai/index.ts`
**Lines**: 135-142

```typescript
const { content: honchoContent } = await honcho.apps.users.sessions.chat(
  appId,
  userId,
  conversationId,
  { queries: honchoQuery }  // Query string generated by "Empath" agent
);
```

**Return Type**: `{ content: string }` - A plain text string, not JSON

**Important Notes**:
- The `queries` parameter is a natural language query string (e.g., "What are the user's learning preferences?")
- This query is generated by an AI agent called "Empath" in the thought stream
- The response is semantic search results from Honcho's memory system
- It returns **already-formatted text**, not structured JSON

### Historical Retrieval

**File**: `utils/ai/conversation.ts`
**Lines**: 35-43

```typescript
honcho.apps.users.metamessages.list(
  appId,
  userId,
  {
    session_id: conversationId,
    metamessage_type: 'honcho',  // Working representations stored here
    reverse: true,
    size: MAX_CONTEXT_SIZE,
  }
)
```

**Return Type**:
```typescript
interface MetaMessage {
  message_id: string | null;
  content: string;  // Plain text, not JSON
}
```

---

## 2. HOW RAW REPRESENTATIONS ARE TRANSFORMED

### Transformation Pipeline

```
Honcho API Response (Plain Text String)
          ↓
    XML Tag Wrapping
          ↓
Injection into LLM Prompt at Specific Positions
```

### Stage 1: Historical Context (Empath Agent)

**File**: `utils/ai/prompts.ts`
**Function**: `buildThoughtPrompt()`
**Lines**: 64-85

```typescript
const honchoResponse = prevUserIndex >= 0
  ? honchoHistory.find(
      (h) => h.message_id === messageHistory[prevUserIndex].id
    )
  : null;

return user`
  <honcho-response>${honchoResponse?.content || 'None'}</honcho-response>
  <pdf-response>${pdfResponse?.content || 'None'}</pdf-response>
  <tutor>${tutorResponse?.content || 'None'}</tutor>
  ${message.content}`;
```

**Transformation**:
- Input: `MetaMessage { message_id: "123", content: "User prefers visual examples" }`
- Output: `<honcho-response>User prefers visual examples</honcho-response>`

### Stage 2: Fresh Context (Bloom Tutor Agent)

**File**: `utils/ai/prompts.ts`
**Function**: `buildResponsePrompt()`
**Lines**: 131-142, 152-154

```typescript
// Historical working representations
const honchoMessage = honchoHistory.find((m) => m.message_id === message.id)?.content || 'No Honcho Message';

responseHistory.push(
  user`<context>${honchoMessage}</context>
  <pdf_context>${pdfMessage}</pdf_context>
  ${message.content}`
);

// Fresh working representation
const mostRecentMessage = user`<context>${honchoContent}</context>
  <pdf_context>${pdfContent}</pdf_context>
  <current_message>${currentMessage}</current_message>`;
```

**Transformation**:
- Input: `{ content: "User struggles with abstract concepts, needs concrete examples" }`
- Output: `<context>User struggles with abstract concepts, needs concrete examples</context>`

---

## 3. WHAT FORMAT IS SENT TO THE LLM

### Format Structure

**For Empath Agent (Thought Generation)**:
```xml
<honcho-response>[PREVIOUS_WORKING_REP]</honcho-response>
<pdf-response>[PDF_CONTEXT]</pdf-response>
<tutor>[PREVIOUS_TUTOR_RESPONSE]</tutor>
[USER_MESSAGE]
```

**For Bloom Agent (Tutor Response)**:
```xml
<context>[WORKING_REPRESENTATION]</context>
<pdf_context>[PDF_CONTEXT]</pdf_context>
<current_message>[USER_MESSAGE]</current_message>
```

### Complete Example Flow

**Step 1 - Empath generates query**:
```
User message: "Can you explain calculus?"
Empath sees: <honcho-response>User prefers visual examples</honcho-response>
Empath output: "What learning style does the user prefer?"␁
```

**Step 2 - Query Honcho**:
```typescript
const { content } = await honcho.apps.users.sessions.chat(appId, userId, conversationId, {
  queries: "What learning style does the user prefer?"
});
// content = "The user prefers visual, hands-on examples and struggles with pure theory"
```

**Step 3 - Inject into Bloom prompt**:
```xml
<context>The user prefers visual, hands-on examples and struggles with pure theory</context>
<current_message>Can you explain calculus?</current_message>
```

**Step 4 - Bloom generates response**:
```
"Let me explain calculus using a visual example. Imagine you're driving a car..."
```

---

## 4. UTILITY FUNCTIONS AND HELPERS

### XML Tag Extraction

**File**: `utils/ai/prompts.ts`
**Lines**: 6-31

```typescript
export function extractTagContent(str: string, tagName: string): string {
  try {
    const match = str.match(
      new RegExp(`<${tagName}>([\\s\\S]*?)</${tagName}>`)
    );
    return match ? match[1].trim() : str;
  } catch {
    return str;
  }
}

export function parseHonchoContent(str: string): string {
  return extractTagContent(str, 'honcho');
}

export function parsePDFContent(str: string): string {
  return extractTagContent(str, 'pdf-agent');
}
```

### Delimiter Parsing

**File**: `utils/ai/index.ts`
**Lines**: 75-130

The system uses the `␁` (Unit Separator) character to delimit sections in the Empath output:

```
[THOUGHT PROCESS]␁[HONCHO_QUERY]␁[PDF_QUERY]
```

Parsing logic:
```typescript
for await (const chunk of thoughtStream) {
  thought += chunk;
  if (chunk.includes('␁')) {
    const segments = chunk.split('␁');

    // First segment is thought
    // Second segment is honcho query
    // Third segment is PDF query

    if (currentSection === 'thought') {
      currentSection = 'honchoQuery';
    } else if (currentSection === 'honchoQuery') {
      currentSection = 'pdfQuery';
    }
  }
}
```

---

## 5. PERSISTENCE PATTERN

### Saving Working Representations

**File**: `utils/ai/conversation.ts`
**Lines**: 122-133

```typescript
await honcho.apps.users.metamessages.create(
  appId,
  userId,
  {
    session_id: conversationId,
    message_id: newUserMessage.id,
    metamessage_type: 'honcho',      // Type identifier
    content: honchoContent || '',     // Plain text string
    metadata: { type: 'assistant' },
  }
);
```

**Storage Schema**:
- `metamessage_type`: 'honcho' (vs 'thought', 'pdf', 'summary', 'collection')
- `content`: Plain text string (NOT JSON)
- `message_id`: Links to the user message it was generated for
- `metadata`: `{ type: 'assistant' }`

---

## 6. COMPLETE REQUEST FLOW

### Full Lifecycle (utils/ai/index.ts)

```typescript
// 1. FETCH HISTORY
const { honchoMessages } = await fetchConversationHistory(appId, userId, conversationId);

// 2. BUILD THOUGHT PROMPT (with historical working reps)
const thoughtPrompt = buildThoughtPrompt(
  messageHistory,
  thoughtHistory,
  honchoMessages,  // Historical working representations
  pdfHistory,
  message,
  hasPDF
);

// 3. GENERATE THOUGHT & PARSE QUERY
const { textStream } = streamText({ messages: thoughtPrompt });
// Parse stream to extract honchoQuery string

// 4. QUERY HONCHO (semantic search)
const { content: honchoContent } = await honcho.apps.users.sessions.chat(
  appId,
  userId,
  conversationId,
  { queries: honchoQuery }
);
// Returns: { content: "User prefers..." } - plain text

// 5. BUILD RESPONSE PROMPT (with fresh working rep)
const responsePrompt = buildResponsePrompt(
  messageHistory,
  honchoMessages,    // Historical reps
  pdfHistory,
  message,
  honchoContent,     // Fresh working rep (plain text)
  pdfContent,
  lastSummary
);

// 6. GENERATE RESPONSE
const { textStream: responseStream } = streamText({ messages: responsePrompt });

// 7. SAVE WORKING REP
await saveConversation(
  appId,
  userId,
  conversationId,
  message,
  thought,
  honchoContent,  // Saved as metamessage type='honcho'
  pdfContent,
  response
);
```

---

## 7. KEY DIFFERENCES FROM CURRENT IMPLEMENTATION

### Current Implementation (app/main.py)

```python
working_rep = get_working_rep(...)  # Returns dict
working_rep_str = json.dumps(working_rep, ensure_ascii=False)
# Sends: {"representation": {"explicit": [...], "deductive": [...]}}
```

### tutor-gpt Implementation

```typescript
const { content } = await honcho.apps.users.sessions.chat(...)
// Returns: { content: "User prefers visual examples..." }

const prompt = `<context>${content}</context>`
// Sends: <context>User prefers visual examples...</context>
```

### Critical Differences

| Aspect | Current (app/main.py) | tutor-gpt |
|--------|----------------------|-----------|
| **API Method** | `get_working_rep()` or similar | `honcho.apps.users.sessions.chat()` |
| **Return Type** | Dictionary/JSON object | Plain text string |
| **Format Sent** | Raw JSON string | XML-wrapped text |
| **Structure** | `{"representation": {...}}` | `<context>...</context>` |
| **Token Efficiency** | Low (includes metadata) | High (clean text) |
| **Readability** | Machine-readable | Human-readable |

---

## 8. RECOMMENDATIONS FOR ADAPTATION

### Option 1: Use Honcho's Chat API

If your Honcho instance supports `sessions.chat()`:

```python
# Generate semantic query with LLM
query = generate_query(conversation_history)

# Query Honcho
response = honcho.apps.users.sessions.chat(
    app_id=app_id,
    user_id=user_id,
    session_id=session_id,
    queries=query
)

# Format for LLM
context = f"<context>{response['content']}</context>"
```

### Option 2: Transform Existing Working Rep

If you must use the existing API:

```python
import json

def format_working_rep_for_llm(working_rep: dict) -> str:
    """Transform working representation into readable text."""
    parts = []

    # Extract explicit facts
    if 'representation' in working_rep:
        rep = working_rep['representation']

        # Process explicit items
        if 'explicit' in rep:
            for item in rep['explicit']:
                # Extract only the content, skip metadata
                if 'content' in item:
                    parts.append(item['content'])

        # Process deductive items
        if 'deductive' in rep:
            for item in rep['deductive']:
                # Extract only the conclusion, skip metadata
                if 'conclusion' in item:
                    parts.append(item['conclusion'])

    # Join into readable text
    return '\n'.join(parts)

# Usage
working_rep = get_working_rep(...)
formatted_text = format_working_rep_for_llm(working_rep)
context = f"<context>{formatted_text}</context>"
```

### Option 3: Hybrid Approach

```python
def create_personalization_context(working_rep: dict, query: str = None) -> str:
    """
    Create LLM-friendly context from working representation.

    Args:
        working_rep: Raw working representation dict
        query: Optional semantic filter query

    Returns:
        Formatted context string
    """
    parts = []

    if 'representation' in working_rep:
        rep = working_rep['representation']

        # Explicit facts (always include)
        for item in rep.get('explicit', []):
            content = item.get('content', '')
            if content and (not query or is_relevant(content, query)):
                parts.append(f"• {content}")

        # Deductive insights (filtered by relevance)
        for item in rep.get('deductive', []):
            conclusion = item.get('conclusion', '')
            if conclusion and (not query or is_relevant(conclusion, query)):
                parts.append(f"• {conclusion}")

    if not parts:
        return "No relevant personalization data available."

    return '\n'.join(parts)

# Usage
working_rep = get_working_rep(...)
context_text = create_personalization_context(working_rep)
prompt = f"""
<context>
{context_text}
</context>

<current_message>{user_message}</current_message>
"""
```

---

## 9. EXAMPLE TRANSFORMATION

### Before (Raw JSON)

```json
{
  "representation": {
    "explicit": [
      {
        "created_at": "2024-01-15T10:30:00Z",
        "message_ids": ["msg_123", "msg_456"],
        "session_names": ["session_1"],
        "content": "User prefers visual examples and diagrams"
      },
      {
        "created_at": "2024-01-15T10:35:00Z",
        "message_ids": ["msg_789"],
        "session_names": ["session_1"],
        "content": "User is studying calculus at university level"
      }
    ],
    "deductive": [
      {
        "created_at": "2024-01-15T10:32:00Z",
        "message_ids": ["msg_123", "msg_456", "msg_789"],
        "session_names": ["session_1"],
        "conclusion": "User likely benefits from step-by-step visual explanations"
      }
    ]
  }
}
```

**Token count**: ~200+ tokens (with metadata)

### After (tutor-gpt Style)

```xml
<context>
User prefers visual examples and diagrams
User is studying calculus at university level
User likely benefits from step-by-step visual explanations
</context>
```

**Token count**: ~25 tokens (8x reduction)

**Benefits**:
- ✅ 8x more token-efficient
- ✅ Easier for LLM to parse
- ✅ No unnecessary metadata
- ✅ Clean, readable format
- ✅ Maintains semantic meaning

---

## 10. CODE LOCATIONS REFERENCE

| Purpose | File | Function | Lines |
|---------|------|----------|-------|
| **Retrieve History** | `utils/ai/conversation.ts` | `fetchConversationHistory()` | 8-85 |
| **Query Honcho** | `utils/ai/index.ts` | `respond()` | 135-142 |
| **Format for Empath** | `utils/ai/prompts.ts` | `buildThoughtPrompt()` | 33-114 |
| **Format for Bloom** | `utils/ai/prompts.ts` | `buildResponsePrompt()` | 116-162 |
| **Parse Delimiters** | `utils/ai/index.ts` | `respond()` | 90-130 |
| **Save Working Rep** | `utils/ai/conversation.ts` | `saveConversation()` | 87-173 |
| **Extract XML Tags** | `utils/ai/prompts.ts` | `extractTagContent()` | 6-14 |

---

## 11. ARCHITECTURAL INSIGHTS

### Two-Agent System

1. **Empath Agent** (Thought Generation)
   - Sees historical working representations
   - Generates semantic queries
   - Outputs: `[thought]␁[honcho_query]␁[pdf_query]`

2. **Bloom Agent** (Response Generation)
   - Sees fresh working representations
   - Uses context to personalize response
   - Outputs: Final tutor response

### Memory Architecture

```
User Message
    ↓
[Empath: Generate Query] ← Historical Working Reps
    ↓
[Honcho Semantic Search] → Fresh Working Rep
    ↓
[Bloom: Generate Response] ← Fresh + Historical Working Reps
    ↓
Save: Message + Thought + Working Rep + Response
```

### XML Tag Conventions

- `<honcho-response>`: Historical context (Empath)
- `<context>`: Fresh personalization (Bloom)
- `<pdf-response>`, `<pdf_context>`: PDF-related context
- `<tutor>`: Previous tutor response
- `<current_message>`: Current user input
- `<past_summary>`: Conversation summary

---

## 12. CONFIGURATION & SETUP

### Environment Variables

```typescript
// utils/honcho.ts
export const honcho = new Honcho({
  baseURL: process.env.HONCHO_URL!,
});
```

Required:
- `HONCHO_URL`: Honcho API endpoint
- `HONCHO_APP_NAME`: Application name

### Caching Strategy

```typescript
export const getHonchoApp = unstable_cache(
  async () => {
    return await honcho.apps.getOrCreate(process.env.HONCHO_APP_NAME!, {
      timeout: 5 * 1000,
      maxRetries: 5,
    });
  },
  [],
  { revalidate: 300 }  // 5-minute cache
);
```

---

## CONCLUSION

tutor-gpt's approach to formatting Honcho working representations is **fundamentally different** from sending raw JSON:

1. **Uses semantic search API** (`sessions.chat()`) that returns plain text
2. **Wraps in XML tags** for structured injection (`<context>...</context>`)
3. **Filters out metadata** (timestamps, IDs) - only semantic content
4. **Token-efficient** (~8x reduction compared to raw JSON)
5. **LLM-friendly** (readable text vs machine data)

The key takeaway: **Don't send JSON - extract the meaningful text and wrap it in XML tags.**
